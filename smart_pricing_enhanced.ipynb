{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Product Pricing - Enhanced Solution with CLIP\n",
    "## Advanced Multimodal Price Prediction\n",
    "\n",
    "**Key Enhancements:**\n",
    "- CLIP-ViT-B/32 for joint image-text embeddings\n",
    "- GroupKFold by brand to prevent data leakage\n",
    "- Advanced post-processing with per-brand bias correction\n",
    "- Dual modeling: Neural Network + LightGBM ensemble\n",
    "- Box-Cox and log1p target transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages for CLIP\n",
    "!pip install -q torch torchvision sentence-transformers pillow pandas numpy scikit-learn lightgbm tqdm requests transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import hashlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Configuration\n",
    "DATA_DIR = Path('dataset')\n",
    "IMAGE_DIR = Path('images')\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CONFIG = {\n",
    "    # CLIP configuration\n",
    "    'use_clip': True,\n",
    "    'clip_model': 'openai/clip-vit-base-patch32',  # CLIP-ViT-B/32\n",
    "    'freeze_clip': True,  # Freeze CLIP weights for efficiency\n",
    "    \n",
    "    # Alternative text model (if not using CLIP text encoder)\n",
    "    'text_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 64,\n",
    "    'epochs': 30,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "    'n_folds': 5,\n",
    "    'early_stopping_rounds': 5,\n",
    "    \n",
    "    # Target transformation\n",
    "    'target_transform': 'log1p',  # 'log1p' or 'boxcox'\n",
    "    \n",
    "    # Post-processing\n",
    "    'use_brand_calibration': True,\n",
    "    'use_median_scaling': True,\n",
    "    'min_price': 0.01,\n",
    "    \n",
    "    # Ensemble\n",
    "    'use_lgb': True,\n",
    "    'nn_weight': 0.7,\n",
    "    'lgb_weight': 0.3,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Group Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "print(f\"\\nPrice statistics:\")\n",
    "print(train_df['price'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(df, method='brand'):\n",
    "    \"\"\"\n",
    "    Create groups for GroupKFold to prevent data leakage\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with catalog_content\n",
    "        method: 'brand' or 'hash'\n",
    "    \n",
    "    Returns:\n",
    "        Array of group identifiers\n",
    "    \"\"\"\n",
    "    if method == 'brand':\n",
    "        # Extract brand from first few words\n",
    "        def extract_brand(text):\n",
    "            if pd.isna(text):\n",
    "                return 'unknown'\n",
    "            words = str(text).split()[:3]\n",
    "            brand = ' '.join(words).split(',')[0].split('-')[0].strip()\n",
    "            return brand.lower() if brand else 'unknown'\n",
    "        \n",
    "        brands = df['catalog_content'].apply(extract_brand)\n",
    "        \n",
    "        # Encode brands\n",
    "        brand_encoder = LabelEncoder()\n",
    "        groups = brand_encoder.fit_transform(brands)\n",
    "        \n",
    "        print(f\"Created {len(np.unique(groups))} brand groups\")\n",
    "        \n",
    "    elif method == 'hash':\n",
    "        # Hash product title for grouping\n",
    "        def hash_title(text):\n",
    "            if pd.isna(text):\n",
    "                return 0\n",
    "            # Take first 50 chars as title proxy\n",
    "            title = str(text)[:50]\n",
    "            # Hash to create groups (modulo to limit number of groups)\n",
    "            hash_val = int(hashlib.md5(title.encode()).hexdigest(), 16)\n",
    "            return hash_val % 1000  # 1000 groups\n",
    "        \n",
    "        groups = df['catalog_content'].apply(hash_title).values\n",
    "        print(f\"Created {len(np.unique(groups))} hash-based groups\")\n",
    "    \n",
    "    return groups\n",
    "\n",
    "# Create groups for training data\n",
    "train_groups = create_groups(train_df, method='brand')\n",
    "\n",
    "print(f\"\\nGroup distribution:\")\n",
    "print(f\"  Min samples per group: {np.bincount(train_groups).min()}\")\n",
    "print(f\"  Max samples per group: {np.bincount(train_groups).max()}\")\n",
    "print(f\"  Mean samples per group: {np.bincount(train_groups).mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Download (same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import download utilities\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from utils import download_images_parallel\n",
    "\n",
    "print(\"Downloading images...\")\n",
    "download_images_parallel(train_df, IMAGE_DIR / 'train', max_workers=20)\n",
    "download_images_parallel(test_df, IMAGE_DIR / 'test', max_workers=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Feature Engineering with Brand Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedStructuredFeatureExtractor:\n",
    "    \"\"\"Enhanced structured feature extraction with brand handling\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.brand_encoder = LabelEncoder()\n",
    "        self.fitted = False\n",
    "        self.brand_stats = {}  # Store per-brand statistics\n",
    "    \n",
    "    def extract_quantity(self, text):\n",
    "        \"\"\"Extract numeric quantities\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 1.0\n",
    "        \n",
    "        patterns = [\n",
    "            r'pack of (\\d+)',\n",
    "            r'(\\d+)[\\s-]*pack',\n",
    "            r'(\\d+)[\\s-]*count',\n",
    "            r'(\\d+)\\s*x\\s*\\d+',\n",
    "            r'IPQ[:\\s]*(\\d+)',\n",
    "            r'quantity[:\\s]*(\\d+)',\n",
    "        ]\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                try:\n",
    "                    return float(match.group(1))\n",
    "                except:\n",
    "                    pass\n",
    "        return 1.0\n",
    "    \n",
    "    def extract_weight(self, text):\n",
    "        \"\"\"Extract weight/volume in grams/ml\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 0.0\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        patterns = [\n",
    "            (r'(\\d+\\.?\\d*)\\s*kg', 1000),\n",
    "            (r'(\\d+\\.?\\d*)\\s*g\\b', 1),\n",
    "            (r'(\\d+\\.?\\d*)\\s*lb', 453.592),\n",
    "            (r'(\\d+\\.?\\d*)\\s*oz', 28.3495),\n",
    "            (r'(\\d+\\.?\\d*)\\s*ml', 1),\n",
    "            (r'(\\d+\\.?\\d*)\\s*l\\b', 1000),\n",
    "        ]\n",
    "        \n",
    "        for pattern, multiplier in patterns:\n",
    "            match = re.search(pattern, text_lower)\n",
    "            if match:\n",
    "                try:\n",
    "                    return float(match.group(1)) * multiplier\n",
    "                except:\n",
    "                    pass\n",
    "        return 0.0\n",
    "    \n",
    "    def extract_brand(self, text):\n",
    "        \"\"\"Extract brand name\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return 'unknown'\n",
    "        \n",
    "        words = text.split()[:3]\n",
    "        brand = ' '.join(words).split(',')[0].split('-')[0].strip()\n",
    "        return brand.lower() if brand else 'unknown'\n",
    "    \n",
    "    def extract_binary_flags(self, text):\n",
    "        \"\"\"Extract binary features\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return {}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        return {\n",
    "            'is_organic': int('organic' in text_lower),\n",
    "            'is_vegan': int('vegan' in text_lower),\n",
    "            'is_gluten_free': int('gluten free' in text_lower or 'gluten-free' in text_lower),\n",
    "            'is_combo': int('combo' in text_lower or 'bundle' in text_lower),\n",
    "            'is_refill': int('refill' in text_lower),\n",
    "            'is_premium': int(any(word in text_lower for word in ['premium', 'deluxe', 'luxury'])),\n",
    "        }\n",
    "    \n",
    "    def fit_transform(self, df, prices=None):\n",
    "        \"\"\"Extract features and compute brand statistics\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        features['quantity'] = df['catalog_content'].apply(self.extract_quantity)\n",
    "        features['weight'] = df['catalog_content'].apply(self.extract_weight)\n",
    "        features['text_length'] = df['catalog_content'].fillna('').apply(len)\n",
    "        features['word_count'] = df['catalog_content'].fillna('').apply(lambda x: len(x.split()))\n",
    "        \n",
    "        # Brand encoding\n",
    "        brands = df['catalog_content'].apply(self.extract_brand)\n",
    "        features['brand'] = self.brand_encoder.fit_transform(brands)\n",
    "        features['brand_name'] = brands.values  # Keep for later\n",
    "        \n",
    "        # Compute per-brand price statistics (for calibration)\n",
    "        if prices is not None:\n",
    "            brand_df = pd.DataFrame({\n",
    "                'brand': brands,\n",
    "                'price': prices\n",
    "            })\n",
    "            self.brand_stats = brand_df.groupby('brand')['price'].agg(['mean', 'median', 'std', 'count']).to_dict('index')\n",
    "            print(f\"Computed statistics for {len(self.brand_stats)} brands\")\n",
    "        \n",
    "        # Binary flags\n",
    "        flags_df = pd.DataFrame(df['catalog_content'].apply(self.extract_binary_flags).tolist())\n",
    "        for col in flags_df.columns:\n",
    "            features[col] = flags_df[col].values\n",
    "        \n",
    "        self.fitted = True\n",
    "        return pd.DataFrame(features)\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform test data\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        features['quantity'] = df['catalog_content'].apply(self.extract_quantity)\n",
    "        features['weight'] = df['catalog_content'].apply(self.extract_weight)\n",
    "        features['text_length'] = df['catalog_content'].fillna('').apply(len)\n",
    "        features['word_count'] = df['catalog_content'].fillna('').apply(lambda x: len(x.split()))\n",
    "        \n",
    "        # Brand encoding with unknown handling\n",
    "        brands = df['catalog_content'].apply(self.extract_brand)\n",
    "        brand_encoded = []\n",
    "        for b in brands:\n",
    "            if b in self.brand_encoder.classes_:\n",
    "                brand_encoded.append(self.brand_encoder.transform([b])[0])\n",
    "            else:\n",
    "                brand_encoded.append(-1)\n",
    "        features['brand'] = brand_encoded\n",
    "        features['brand_name'] = brands.values\n",
    "        \n",
    "        # Binary flags\n",
    "        flags_df = pd.DataFrame(df['catalog_content'].apply(self.extract_binary_flags).tolist())\n",
    "        for col in flags_df.columns:\n",
    "            features[col] = flags_df[col].values\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "\n",
    "# Extract structured features\n",
    "print(\"Extracting structured features...\")\n",
    "struct_extractor = EnhancedStructuredFeatureExtractor()\n",
    "train_struct = struct_extractor.fit_transform(train_df, train_df['price'].values)\n",
    "test_struct = struct_extractor.transform(test_df)\n",
    "\n",
    "print(f\"Features: {[c for c in train_struct.columns if c != 'brand_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CLIP Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPFeatureExtractor:\n",
    "    \"\"\"Extract features using frozen CLIP-ViT-B/32\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='openai/clip-vit-base-patch32', freeze=True):\n",
    "        print(f\"Loading CLIP model: {model_name}\")\n",
    "        self.model = CLIPModel.from_pretrained(model_name)\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        \n",
    "        if freeze:\n",
    "            # Freeze all CLIP parameters\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"CLIP weights frozen\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        self.model.to(device)\n",
    "    \n",
    "    def extract_image_features(self, sample_ids, image_dir, batch_size=64):\n",
    "        \"\"\"Extract image features using CLIP vision encoder\"\"\"\n",
    "        features_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(sample_ids), batch_size), desc=\"CLIP image features\"):\n",
    "                batch_ids = sample_ids[i:i+batch_size]\n",
    "                batch_images = []\n",
    "                \n",
    "                for sample_id in batch_ids:\n",
    "                    img_path = Path(image_dir) / f\"{sample_id}.jpg\"\n",
    "                    try:\n",
    "                        img = Image.open(img_path).convert('RGB')\n",
    "                    except:\n",
    "                        # Create blank image if loading fails\n",
    "                        img = Image.new('RGB', (224, 224), color='black')\n",
    "                    batch_images.append(img)\n",
    "                \n",
    "                # Process with CLIP\n",
    "                inputs = self.processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Get image features\n",
    "                image_features = self.model.get_image_features(**inputs)\n",
    "                features_list.append(image_features.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(features_list)\n",
    "    \n",
    "    def extract_text_features(self, texts, batch_size=64):\n",
    "        \"\"\"Extract text features using CLIP text encoder\"\"\"\n",
    "        texts = [str(t) if not pd.isna(t) else '' for t in texts]\n",
    "        features_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(texts), batch_size), desc=\"CLIP text features\"):\n",
    "                batch_texts = texts[i:i+batch_size]\n",
    "                \n",
    "                # Truncate long texts (CLIP has 77 token limit)\n",
    "                batch_texts = [t[:500] for t in batch_texts]\n",
    "                \n",
    "                inputs = self.processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Get text features\n",
    "                text_features = self.model.get_text_features(**inputs)\n",
    "                features_list.append(text_features.cpu().numpy())\n",
    "        \n",
    "        return np.vstack(features_list)\n",
    "\n",
    "if CONFIG['use_clip']:\n",
    "    print(\"\\nExtracting CLIP features...\")\n",
    "    clip_extractor = CLIPFeatureExtractor(CONFIG['clip_model'], freeze=CONFIG['freeze_clip'])\n",
    "    \n",
    "    # Extract image features\n",
    "    train_image = clip_extractor.extract_image_features(train_df['sample_id'].values, IMAGE_DIR / 'train')\n",
    "    test_image = clip_extractor.extract_image_features(test_df['sample_id'].values, IMAGE_DIR / 'test')\n",
    "    print(f\"Image features: {train_image.shape}\")\n",
    "    \n",
    "    # Extract text features\n",
    "    train_text = clip_extractor.extract_text_features(train_df['catalog_content'].values)\n",
    "    test_text = clip_extractor.extract_text_features(test_df['catalog_content'].values)\n",
    "    print(f\"Text features: {train_text.shape}\")\n",
    "    \n",
    "    # Note: CLIP features are already L2-normalized\n",
    "else:\n",
    "    # Fallback to separate encoders\n",
    "    print(\"\\nUsing separate text/image encoders...\")\n",
    "    # (Use previous implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale structured features\n",
    "print(\"\\nScaling structured features...\")\n",
    "struct_cols = [c for c in train_struct.columns if c != 'brand_name']\n",
    "struct_scaler = StandardScaler()\n",
    "train_struct_scaled = struct_scaler.fit_transform(train_struct[struct_cols])\n",
    "test_struct_scaled = struct_scaler.transform(test_struct[struct_cols])\n",
    "\n",
    "print(f\"Structured features: {train_struct_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Enhanced Target Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTargetTransformer:\n",
    "    \"\"\"Enhanced target transformation with Box-Cox option\"\"\"\n",
    "    \n",
    "    def __init__(self, method='log1p'):\n",
    "        self.method = method\n",
    "        self.boxcox_lambda = None\n",
    "        self.shift = 1e-6\n",
    "    \n",
    "    def fit_transform(self, y):\n",
    "        \"\"\"Fit and transform target\"\"\"\n",
    "        if self.method == 'log1p':\n",
    "            return np.log1p(y)\n",
    "        elif self.method == 'boxcox':\n",
    "            y_shifted = y + self.shift\n",
    "            # Find optimal lambda\n",
    "            transformed, self.boxcox_lambda = stats.boxcox(y_shifted)\n",
    "            print(f\"Box-Cox lambda: {self.boxcox_lambda:.4f}\")\n",
    "            return transformed\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def inverse_transform(self, y_transformed):\n",
    "        \"\"\"Inverse transform (expm1 or Box-Cox inverse)\"\"\"\n",
    "        if self.method == 'log1p':\n",
    "            return np.expm1(y_transformed)\n",
    "        elif self.method == 'boxcox':\n",
    "            y_original = stats.inv_boxcox(y_transformed, self.boxcox_lambda)\n",
    "            return y_original - self.shift\n",
    "        else:\n",
    "            return y_transformed\n",
    "\n",
    "# Transform target\n",
    "print(f\"\\nApplying {CONFIG['target_transform']} transformation...\")\n",
    "target_transformer = EnhancedTargetTransformer(method=CONFIG['target_transform'])\n",
    "train_target = target_transformer.fit_transform(train_df['price'].values)\n",
    "\n",
    "print(f\"Original range: [{train_df['price'].min():.2f}, {train_df['price'].max():.2f}]\")\n",
    "print(f\"Transformed range: [{train_target.min():.2f}, {train_target.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compact Multimodal Fusion MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompactMultimodalMLP(nn.Module):\n",
    "    \"\"\"Compact fusion MLP: concatenate text + image + structured features\"\"\"\n",
    "    \n",
    "    def __init__(self, text_dim, image_dim, structured_dim, hidden_dims=[256, 128, 64]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Concatenate all features directly\n",
    "        input_dim = text_dim + image_dim + structured_dim\n",
    "        \n",
    "        # Build MLP\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, text_features, image_features, structured_features):\n",
    "        # Simple concatenation\n",
    "        combined = torch.cat([text_features, image_features, structured_features], dim=1)\n",
    "        output = self.mlp(combined)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset and SMAPE Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, text_features, image_features, structured_features, targets=None):\n",
    "        self.text_features = torch.FloatTensor(text_features)\n",
    "        self.image_features = torch.FloatTensor(image_features)\n",
    "        self.structured_features = torch.FloatTensor(structured_features)\n",
    "        self.targets = torch.FloatTensor(targets) if targets is not None else None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return (\n",
    "                self.text_features[idx],\n",
    "                self.image_features[idx],\n",
    "                self.structured_features[idx],\n",
    "                self.targets[idx]\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                self.text_features[idx],\n",
    "                self.image_features[idx],\n",
    "                self.structured_features[idx]\n",
    "            )\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"SMAPE metric\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    denominator = np.where(denominator == 0, 1e-8, denominator)\n",
    "    return np.mean(np.abs(y_true - y_pred) / denominator) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training with GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_fold(model, train_loader, val_loader, epochs, lr, target_transformer, patience=5):\n",
    "    \"\"\"Train neural network for one fold\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    best_val_smape = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            text_feat, image_feat, struct_feat, targets = batch\n",
    "            text_feat = text_feat.to(device)\n",
    "            image_feat = image_feat.to(device)\n",
    "            struct_feat = struct_feat.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text_feat, image_feat, struct_feat)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                text_feat, image_feat, struct_feat, targets = batch\n",
    "                text_feat = text_feat.to(device)\n",
    "                image_feat = image_feat.to(device)\n",
    "                struct_feat = struct_feat.to(device)\n",
    "                \n",
    "                outputs = model(text_feat, image_feat, struct_feat)\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.numpy())\n",
    "        \n",
    "        # Calculate SMAPE on original scale\n",
    "        val_preds_orig = target_transformer.inverse_transform(np.array(val_preds))\n",
    "        val_targets_orig = target_transformer.inverse_transform(np.array(val_targets))\n",
    "        val_smape = smape(val_targets_orig, val_preds_orig)\n",
    "        \n",
    "        scheduler.step(val_smape)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{epochs} - Loss: {train_loss:.4f} - SMAPE: {val_smape:.2f}%\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_smape < best_val_smape:\n",
    "            best_val_smape = val_smape\n",
    "            best_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, best_val_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupKFold Cross-Validation\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GroupKFold Cross-Validation (n_splits={CONFIG['n_folds']})\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "gkf = GroupKFold(n_splits=CONFIG['n_folds'])\n",
    "nn_models = []\n",
    "nn_scores = []\n",
    "lgb_models = []\n",
    "lgb_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups=train_groups), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/{CONFIG['n_folds']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_text = train_text[train_idx]\n",
    "    X_val_text = train_text[val_idx]\n",
    "    X_train_image = train_image[train_idx]\n",
    "    X_val_image = train_image[val_idx]\n",
    "    X_train_struct = train_struct_scaled[train_idx]\n",
    "    X_val_struct = train_struct_scaled[val_idx]\n",
    "    y_train = train_target[train_idx]\n",
    "    y_val = train_target[val_idx]\n",
    "    \n",
    "    # === Neural Network ===\n",
    "    print(\"\\nTraining Neural Network...\")\n",
    "    train_dataset = PriceDataset(X_train_text, X_train_image, X_train_struct, y_train)\n",
    "    val_dataset = PriceDataset(X_val_text, X_val_image, X_val_struct, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    \n",
    "    nn_model = CompactMultimodalMLP(\n",
    "        text_dim=train_text.shape[1],\n",
    "        image_dim=train_image.shape[1],\n",
    "        structured_dim=train_struct_scaled.shape[1]\n",
    "    ).to(device)\n",
    "    \n",
    "    nn_model, nn_smape = train_nn_fold(\n",
    "        nn_model, train_loader, val_loader,\n",
    "        epochs=CONFIG['epochs'],\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        target_transformer=target_transformer,\n",
    "        patience=CONFIG['early_stopping_rounds']\n",
    "    )\n",
    "    \n",
    "    nn_models.append(nn_model)\n",
    "    nn_scores.append(nn_smape)\n",
    "    print(f\"NN SMAPE: {nn_smape:.2f}%\")\n",
    "    \n",
    "    # === LightGBM (optional) ===\n",
    "    if CONFIG['use_lgb']:\n",
    "        print(\"\\nTraining LightGBM...\")\n",
    "        X_train_all = np.hstack([X_train_text, X_train_image, X_train_struct])\n",
    "        X_val_all = np.hstack([X_val_text, X_val_image, X_val_struct])\n",
    "        \n",
    "        lgb_params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'mse',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'seed': SEED\n",
    "        }\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train_all, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val_all, label=y_val, reference=train_data)\n",
    "        \n",
    "        lgb_model = lgb.train(\n",
    "            lgb_params,\n",
    "            train_data,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[val_data],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        # Validate LGB\n",
    "        lgb_preds = lgb_model.predict(X_val_all)\n",
    "        lgb_preds_orig = target_transformer.inverse_transform(lgb_preds)\n",
    "        y_val_orig = target_transformer.inverse_transform(y_val)\n",
    "        lgb_smape = smape(y_val_orig, lgb_preds_orig)\n",
    "        \n",
    "        lgb_models.append(lgb_model)\n",
    "        lgb_scores.append(lgb_smape)\n",
    "        print(f\"LGB SMAPE: {lgb_smape:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Cross-Validation Results\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"NN Mean SMAPE: {np.mean(nn_scores):.2f}% (+/- {np.std(nn_scores):.2f}%)\")\n",
    "if CONFIG['use_lgb']:\n",
    "    print(f\"LGB Mean SMAPE: {np.mean(lgb_scores):.2f}% (+/- {np.std(lgb_scores):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating test predictions...\")\n",
    "test_dataset = PriceDataset(test_text, test_image, test_struct_scaled)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "# Neural Network predictions\n",
    "nn_predictions_all = []\n",
    "for fold_idx, model in enumerate(nn_models, 1):\n",
    "    print(f\"  NN Fold {fold_idx}...\")\n",
    "    model.eval()\n",
    "    fold_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            text_feat, image_feat, struct_feat = batch\n",
    "            text_feat = text_feat.to(device)\n",
    "            image_feat = image_feat.to(device)\n",
    "            struct_feat = struct_feat.to(device)\n",
    "            \n",
    "            outputs = model(text_feat, image_feat, struct_feat)\n",
    "            fold_preds.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    nn_predictions_all.append(fold_preds)\n",
    "\n",
    "nn_predictions = np.mean(nn_predictions_all, axis=0)\n",
    "\n",
    "# LightGBM predictions\n",
    "if CONFIG['use_lgb']:\n",
    "    print(\"\\nGenerating LightGBM predictions...\")\n",
    "    X_test_all = np.hstack([test_text, test_image, test_struct_scaled])\n",
    "    lgb_predictions_all = [model.predict(X_test_all) for model in lgb_models]\n",
    "    lgb_predictions = np.mean(lgb_predictions_all, axis=0)\n",
    "    \n",
    "    # Ensemble\n",
    "    print(f\"\\nEnsembling: {CONFIG['nn_weight']*100:.0f}% NN + {CONFIG['lgb_weight']*100:.0f}% LGB\")\n",
    "    final_predictions = (CONFIG['nn_weight'] * nn_predictions + \n",
    "                         CONFIG['lgb_weight'] * lgb_predictions)\n",
    "else:\n",
    "    final_predictions = nn_predictions\n",
    "\n",
    "# Inverse transform\n",
    "final_predictions = target_transformer.inverse_transform(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_predictions(predictions, min_val=0.01):\n",
    "    \"\"\"Clip negative predictions to minimum value\"\"\"\n",
    "    return np.clip(predictions, min_val, None)\n",
    "\n",
    "def median_scaling(predictions, train_prices, max_adjustment=0.1):\n",
    "    \"\"\"Apply median scaling to reduce systematic bias\"\"\"\n",
    "    pred_median = np.median(predictions)\n",
    "    train_median = np.median(train_prices)\n",
    "    scale_factor = train_median / pred_median\n",
    "    \n",
    "    # Limit adjustment\n",
    "    scale_factor = np.clip(scale_factor, 1-max_adjustment, 1+max_adjustment)\n",
    "    \n",
    "    return predictions * scale_factor, scale_factor\n",
    "\n",
    "def per_brand_calibration(predictions, brand_names, brand_stats, alpha=0.5):\n",
    "    \"\"\"Apply per-brand bias correction\"\"\"\n",
    "    calibrated = predictions.copy()\n",
    "    \n",
    "    for i, brand in enumerate(brand_names):\n",
    "        if brand in brand_stats:\n",
    "            stats = brand_stats[brand]\n",
    "            if stats['count'] >= 10:  # Only calibrate if enough samples\n",
    "                # Blend prediction with brand mean\n",
    "                brand_mean = stats['mean']\n",
    "                calibrated[i] = alpha * predictions[i] + (1 - alpha) * brand_mean\n",
    "    \n",
    "    return calibrated\n",
    "\n",
    "# Apply post-processing\n",
    "print(\"\\nApplying post-processing...\")\n",
    "\n",
    "# 1. Clip negatives\n",
    "final_predictions = clip_predictions(final_predictions, min_val=CONFIG['min_price'])\n",
    "print(f\"✓ Clipped predictions to ≥ {CONFIG['min_price']}\")\n",
    "\n",
    "# 2. Median scaling (optional)\n",
    "if CONFIG['use_median_scaling']:\n",
    "    final_predictions, scale_factor = median_scaling(final_predictions, train_df['price'].values)\n",
    "    print(f\"✓ Applied median scaling (factor: {scale_factor:.4f})\")\n",
    "\n",
    "# 3. Per-brand calibration (optional)\n",
    "if CONFIG['use_brand_calibration'] and struct_extractor.brand_stats:\n",
    "    final_predictions = per_brand_calibration(\n",
    "        final_predictions,\n",
    "        test_struct['brand_name'].values,\n",
    "        struct_extractor.brand_stats,\n",
    "        alpha=0.9  # 90% model, 10% brand prior\n",
    "    )\n",
    "    print(f\"✓ Applied per-brand calibration\")\n",
    "\n",
    "print(f\"\\nFinal predictions:\")\n",
    "print(f\"  Min: ${final_predictions.min():.2f}\")\n",
    "print(f\"  Max: ${final_predictions.max():.2f}\")\n",
    "print(f\"  Mean: ${final_predictions.mean():.2f}\")\n",
    "print(f\"  Median: ${np.median(final_predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': final_predictions\n",
    "})\n",
    "\n",
    "print(\"\\nSubmission format:\")\n",
    "print(submission.head(10))\n",
    "print(f\"\\nShape: {submission.shape}\")\n",
    "print(f\"Missing: {submission.isnull().sum().sum()}\")\n",
    "print(f\"Negative: {(submission['price'] < 0).sum()}\")\n",
    "\n",
    "submission.to_csv('test_out.csv', index=False)\n",
    "print(\"\\n✓ Submission saved: test_out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ENHANCED SOLUTION COMPLETE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"✓ Features:\")\n",
    "print(f\"  - CLIP Text: {train_text.shape[1]}D\")\n",
    "print(f\"  - CLIP Image: {train_image.shape[1]}D\")\n",
    "print(f\"  - Structured: {train_struct_scaled.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n✓ Validation (GroupKFold by brand):\")\n",
    "print(f\"  - NN Mean SMAPE: {np.mean(nn_scores):.2f}% (+/- {np.std(nn_scores):.2f}%)\")\n",
    "if CONFIG['use_lgb']:\n",
    "    print(f\"  - LGB Mean SMAPE: {np.mean(lgb_scores):.2f}% (+/- {np.std(lgb_scores):.2f}%)\")\n",
    "\n",
    "print(f\"\\n✓ Post-Processing:\")\n",
    "print(f\"  - Target transform: {CONFIG['target_transform']}\")\n",
    "print(f\"  - Clipping: predictions ≥ ${CONFIG['min_price']}\")\n",
    "if CONFIG['use_median_scaling']:\n",
    "    print(f\"  - Median scaling: applied\")\n",
    "if CONFIG['use_brand_calibration']:\n",
    "    print(f\"  - Brand calibration: applied\")\n",
    "\n",
    "print(f\"\\n✓ Submission: test_out.csv ({len(submission)} predictions)\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
